{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    # A linear layer of shape (d_in × d_out) with (tp_in × tp_out)-degree tensor\n",
    "    # parallelism, dp-degree data parallelism, and bytewidth precision.\n",
    "    def __init__(self, d_in, d_out, tp_in, tp_out, dp, bytewidth):\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.tp_in = tp_in\n",
    "        self.tp_out = tp_out\n",
    "        self.dp = dp\n",
    "        self.bytewidth = bytewidth\n",
    "\n",
    "        # Derived dimensions.\n",
    "        self.N = d_in*d_out                             # number of parameters\n",
    "        self.d_in_local = d_in/tp_in                    # local input dimension\n",
    "        self.d_out_local = d_out/tp_out                 # local output dimension\n",
    "        self.N_local = self.d_in_local*self.d_out_local # local number of parameters\n",
    "\n",
    "class Device:\n",
    "    def __init__(self, name, flop_per_sec_8bit, global_Bps, net_Bps,\n",
    "                 base_util=0.8):\n",
    "        self.name = name\n",
    "        self.flop_per_sec_8bit = flop_per_sec_8bit\n",
    "        self.global_Bps = global_Bps\n",
    "        self.net_Bps = net_Bps\n",
    "        self.base_util = base_util\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def all_reduce_communication(self, n, p):\n",
    "        # The number of words this device has to send or receive to participate\n",
    "        # in an all-reduce of a vector of local size n, with p - 1 other\n",
    "        # devices.\n",
    "        #\n",
    "        # We assume a bandwidth-optimal reduce-scatter + multicast communication\n",
    "        # pattern, where each device is responsible for reducing n/p words:\n",
    "        #\n",
    "        #  Step 1. Transmit p - 1 parts of size n/p to the devices responsible\n",
    "        #          for reducing those parts.\n",
    "        #\n",
    "        #  Step 2. Receive p - 1 copies of the part of size n/p that I'm\n",
    "        #          responsible for reducing from my peers.\n",
    "        #\n",
    "        #  Step 3. Reduce the p copies of my part and multicast the result to my\n",
    "        #          peers (optimistically assumed to be free for simplicity).\n",
    "        #\n",
    "        #  Step 4. Receive p - 1 parts of size n/p that my peers were\n",
    "        #          responsible for reducing.\n",
    "        return 3*(p - 1)*n/p\n",
    "\n",
    "    def linear_layer_fwd_bwd_secs(self, lin_layer, b):\n",
    "        # Determine the time to compute a forward and backward pass through\n",
    "        # lin_layer on a microbatch of b tokens. We take into account the time\n",
    "        # of tensor FLOP, global memory IO, and tensor-parallel all-reduce\n",
    "        # communication.\n",
    "        #\n",
    "        # Some assumptions (all \"optimistic\"):\n",
    "        #  - Data movement and computation are fully overlappable, via the\n",
    "        #    mechanism of multiple microbatches simultaneously in flight.\n",
    "        #\n",
    "        #  - Pipeline-parallel communication time is negligible (though deep\n",
    "        #    pipelines may cause a bubble, which is modeled elsewhere).\n",
    "        #\n",
    "        #  - Pointwise operations such as activation functions and\n",
    "        #    normalizations are negligible. So is attention and any\n",
    "        #    communication it requires.\n",
    "        #\n",
    "        #  - b/dp is substantially smaller than d_in/tp_in and d_out/tp_out,\n",
    "        #    where dp is the data parallel degree, tp_in is the input tensor\n",
    "        #    parallel degree, and tp_out is the output tensor parallel degree.\n",
    "        #    This is because this tends to be communication-optimal due to the\n",
    "        #    significant communication costs of tensor parallelism compared to\n",
    "        #    data parallelism. This in turn has a few implications:\n",
    "        #\n",
    "        #     1. The input and output matrix tends to be quite rectangular,\n",
    "        #        which puts the on-chip data movement bottleneck at global\n",
    "        #        memory rather than shared memory banks, L2 cache, or the\n",
    "        #        SM-to-SM DSMEM network. This is because the SM- and warp-level\n",
    "        #        tiles can still be large and approximately square-shaped, which\n",
    "        #        removes these levels of the memory hierarchy as a bottleneck,\n",
    "        #        however there isn't freedom to do this at the global level.\n",
    "        #        Thus we do not model data movement on-chip except to/from\n",
    "        #        global memory.\n",
    "        #\n",
    "        #     2. We assume activations and their gradients fit in L2 cache when\n",
    "        #        they've recently been accessed (e.g. by a recent network\n",
    "        #        receive, all-reduce, or the previous layer's matmul), but\n",
    "        #        weights and their gradients cannot. Thus we only model global\n",
    "        #        memory IO for weights and gradients, except on the backward\n",
    "        #        pass where activations must be reloaded from global memory\n",
    "        #        (where they are presumed to fit, avoiding the need for\n",
    "        #        activation recomputation).\n",
    "        #\n",
    "        #  - The all-reduce communication pattern is a bandwidth-optimal\n",
    "        #    reduce-scatter followed by a multicast, and we treat the multicast\n",
    "        #    transmission as free for simplicity.\n",
    "        #\n",
    "        # We also don't worry about whether our parallelism degrees divide the\n",
    "        # tensor dimensions.\n",
    "        d_in_local, d_out_local = lin_layer.d_in_local, lin_layer.d_out_local\n",
    "        N_local = lin_layer.N_local\n",
    "        tp_in, tp_out, dp = lin_layer.tp_in, lin_layer.tp_out, lin_layer.dp\n",
    "        bytewidth = lin_layer.bytewidth\n",
    "        b_local = b/dp\n",
    "\n",
    "        flop, global_io, net_io = 0, 0, 0\n",
    "\n",
    "        # Forward pass.\n",
    "        global_io += N_local                                                   # Load weights.\n",
    "        flop      += 2*N_local*b_local                                         # Compute output activations: Y = WX.\n",
    "        net_io    += self.all_reduce_communication(d_out_local*b_local, tp_in) # All-reduce activations (network IO).\n",
    "        global_io += self.all_reduce_communication(d_out_local*b_local, tp_in) # All-reduce activations (global memory IO).\n",
    "\n",
    "        # Backward pass (weight gradients).\n",
    "        global_io += d_in_local*b_local                                        # Reload input activations from global memory.\n",
    "        flop      += 2*N_local*b_local                                         # Compute weight gradients: dL/dW = (dL/dY)X^T.\n",
    "        global_io += 2*N_local                                                 # Accumulate weight gradients (read + write).\n",
    "    \n",
    "        # Backward pass (activation gradients).\n",
    "        global_io += N_local                                                   # Load weights.\n",
    "        flop      += 2*N_local*b_local                                         # Compute input gradients: dL/dX = W^T(dL/dY).\n",
    "        net_io    += self.all_reduce_communication(d_in_local*b_local, tp_out) # All-reduce input gradients (network IO).\n",
    "        global_io += self.all_reduce_communication(d_in_local*b_local, tp_out) # All-reduce input gradients (global memory IO).\n",
    "\n",
    "        # Total time. base_util due to thermal throttling is presumed to affect\n",
    "        # FLOP/s and global memory B/s, but not net B/s.\n",
    "        flop_secs = (flop*bytewidth)/(self.base_util*self.flop_per_sec_8bit)\n",
    "        global_io_secs = (global_io*bytewidth)/(self.base_util*self.global_Bps)\n",
    "        net_io_secs = (net_io*bytewidth)/self.net_Bps\n",
    "        return np.maximum(flop_secs, np.maximum(global_io_secs, net_io_secs))\n",
    "\n",
    "    def linear_layer_end_of_batch_secs(self, lin_layer):\n",
    "        # Determine the time for the end-of-batch gradient all-reduce (we assume\n",
    "        # the optimizer step itself is negligible).\n",
    "        io = self.all_reduce_communication(lin_layer.N_local, lin_layer.dp)\n",
    "        global_io_secs = (io*lin_layer.bytewidth)/(self.base_util*self.global_Bps)\n",
    "        net_io_secs = (io*lin_layer.bytewidth)/self.net_Bps\n",
    "        return np.maximum(global_io_secs, net_io_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2554072481051035e-05"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = Device('H100 SXM5', 1979e12, 3352e9, 900e9)\n",
    "dev.linear_layer_fwd_bwd_secs(bytewidth=2, d_in=16384, d_out=5*16384, tp_in=8, tp_out=40, b=2048**2/128, dp=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
